# Скрипт для обхода сайта с crawl4ai

Обходит сайт и сохраняет контент страниц в .md файлы для дальнейшего использования в AI.

**Особенности:**
- Обход только в пределах указанного домена (BFS)
- Древовидная структура файлов, отражающая URL
- Логирование и статистика
- Автоматическое фильтрование нерелевантного контента

## Установка

```bash
pip install -r requirements.txt
crawl4ai-setup
playwright install chromium
```

## Использование

```bash
python crawler.py -u <URL> -s <SITE_CODE> [-m <MAX_PAGES>] [-o <OUTPUT_DIR>]
```

### Аргументы

- `-u, --base-url` — стартовый URL сайта (обязательно)
- `-s, --site-code` — произвольный префикс для имен файлов, например: `example`, `mysite`, `shop` (обязательно)
- `-m, --max-pages` — максимум страниц (по умолчанию: 50)
- `-o, --output-dir` — директория для сохранения (по умолчанию: `./output`)

**Примечание:** `site-code` — это просто название, которое вы придумываете сами. Оно используется как префикс в имени файлов для удобной организации.

### Примеры

```bash
# Тестовый запуск (site-code = "test")
python crawler.py -u https://example.com -s test -m 10

# Полный обход (site-code = "example")
python crawler.py -u https://example.com -s example -m 100 -o ./results

# Реальный пример (site-code можно выбрать любой, например название сайта)
python crawler.py -u https://mysite.com -s mysite -m 50
```

## Формат выходных файлов

Все файлы сохраняются в одну папку с уникальными именами:

```
output/
  <site-code>/
    <site-code>_index_<hash>.md
    <site-code>_about_<hash>.md
    <site-code>_article_<hash>.md
```

Каждый файл начинается с метаданных (URL и заголовок):

```markdown
---
URL: https://example.com/about
Заголовок: О нас
---

[Содержимое страницы в markdown...]
```

Имена файлов формируются на основе:
- Префикс `site-code`
- Последняя часть пути URL
- Хеш URL для уникальности

## Логирование

Логи сохраняются в `output/logs/<site-code>.log` (уровень DEBUG). В консоль выводится информация уровня INFO.

## Статистика

По завершении обхода выводится статистика:
- Количество обработанных страниц (успешно/ошибки)
- Количество сохраненных файлов
- Найдено/добавлено ссылок
- Время работы
- Типы ошибок
